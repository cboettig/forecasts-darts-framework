---
pagetitle: ML Forecasts for Aquatics Challenge
toc-title: ML Forecasts for Aquatics Challenge
---


```{python}
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
pl_trainer_kwargs={"accelerator": "gpu", "gpus": 1, "auto_select_gpus": True} 

```

``` {python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from darts import TimeSeries
from darts.datasets import AirPassengersDataset
```


``` {python}
aquatics = pd.read_csv("https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz") 
aquatics
```

``` {python}
temp = aquatics.query("site_id == 'BARC'").query("variable=='temperature'").filter(items=['datetime', 'observation'])
```


``` {python}

series = TimeSeries.from_dataframe(temp, time_col = 'datetime', value_cols='observation', fill_missing_dates=True, freq='D')
```


``` {python}
plt.cla()
series.plot()
plt.show()
```



``` {python}
train, val = series.split_before(0.75)
train.plot(label="training")
val.plot(label="validation")
```

``` {python}
horizon = val.n_timesteps
pts_in_yr = 400
```

``` {python}
from darts.models import NaiveSeasonal

seasonal_model = NaiveSeasonal(K=pts_in_yr)
seasonal_model.fit(train)
seasonal_forecast = seasonal_model.predict(horizon)


plt.cla()
series.plot(label="actual")
seasonal_forecast.plot(label="naive forecast (K=12)")
plt.show()

```


# Probablistic Forecasts

``` {python}
## hmm ES fails to converge with defaults
from darts.models import ExponentialSmoothing, Prophet # not probablistic: , AutoARIMA, Theta
model = Prophet()
model.fit(train)
probabilistic_forecast = model.predict(len(val), num_samples=50)

```

```{python}
plt.close()
plot.cla() # clear
series.plot(label="actual")
probabilistic_forecast.plot(label="probabilistic forecast")
plt.legend()
plt.show()

```


Missing values will create a problem for the neural net. We infer them
first:


``` {python}

## Really dumb
from darts.dataprocessing.transformers import MissingValuesFiller
transformer = MissingValuesFiller()
filtered_series = transformer.transform(train)
```


Alternatively, we can fill missing data and smooth the process using a filter such as a Gaussian Process. (not working)

``` {python}
from darts.models import GaussianProcessFilter
from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel
kernel = RBF(length_scale=1) + WhiteKernel(noise_level=1)
gpf = GaussianProcessFilter(kernel=kernel, normalize_y=True)
filtered_series = gpf.filter(train, num_samples=5)
```

``` {python}
plt.cla()
filtered_series.plot()
plt.show()
```

We are now ready to define a deep learning forecasting model. 
Note that the use of a GPU here will dramatically reduce computation time.

``` {python}
from darts.models import TCNModel
from darts.utils.likelihood_models import LaplaceLikelihood


model = TCNModel(
    input_chunk_length=400,  # pts_in_yr
    output_chunk_length=100, # forecast horizon
    random_state=42,
    likelihood=LaplaceLikelihood(),
    pl_trainer_kwargs={"accelerator": "gpu", "gpus": 1, "auto_select_gpus": True} 

)
model.trainer_params
```
Before we can train the model, we must transform the data appropriately.

``` {python}
from darts.dataprocessing.transformers import Scaler
scaler = Scaler()
train_scaled = scaler.fit_transform(filtered_series)
```

And here we go.  It is possible to track performance using Tensorboard. Adjust epoch accordingly for convergence.

``` {python}
model.fit(train_scaled, epochs=400)
```

``` {python}
pred = model.predict(n=horizon, num_samples=30)
```

``` {python}
plt.cla()

pred.plot(low_quantile=0.01, high_quantile=0.99, label="1-99th percentiles")
pred.plot(low_quantile=0.2, high_quantile=0.8, label="20-80th percentiles")

plt.show()
```


``` {python}
plt.cla()

train.plot(label="training")
val.plot(label="validation")
pred = scaler.inverse_transform(pred)
pred.plot(low_quantile=0.01, high_quantile=0.99, label="1-99th percentiles")
pred.plot(low_quantile=0.2, high_quantile=0.8, label="20-80th percentiles")

plt.show()
```


## Standardize to EFI tabular format and submit!


``` {python}
# some simple metadata methods for darts.Timeseries: 
pred.is_stochastic # True
pred.n_components # 1
pred.n_samples # 30
```

Now we would like to serialize our forecast to a standard EFI format
(`datetime`, `site_id`, `variable`, `prediction`, `parameter`)


``` {python}
nd = pred.all_values() # numpy array: time x variables x replicates
var1 = nd[:,0,:] # index the first (only) variable
df = pd.DataFrame(var1)
df['datetime'] = pred.time_index
# pivot longer, ensemble as id, not as column name
df = df.melt(id_vars="datetime", var_name="parameter", value_name="prediction")

from datetime import date
today = date.today()
model_id = "dartsTCN"

df['variable'] = "temperature"
df['site_id'] = "BART"
df['reference_datetime'] = str(today)
df['model_id'] = model_id
df['family'] = "sample"
df
```
And we're ready to submit

```{python}
from utils.forecast_utils import submit
submit(df, "aquatics", model_id)
```









## timeseries in xarray


``` {python}
da = pred.data_array() # underlying xarray object
da = da.assign_coords({"datetime": pred.time_index})

da = da.assign_coords({"site_id": "BARC"}) # text-valued coordinates!
da = da.rename({"sample": "parameter"})
da = da.rename({"component": "variable"})
da.variable.attrs["long name"] = "lake surface temperature"
da.variable.attrs["units"] = "degrees C"
da.attrs["long name"] = "Forecast of temperature levels at BARC site using a ML-based prediction"
da = da.assign_coords({"datetime": pred.time_index})

# must remove unused vars
del da.variable.attrs['static_covariates']
del da.variable.attrs['hierarchy']

da.to_netcdf("test.ncdf")
```


``` {python}
#import xarray as xr
#template = xr.DataArray(da.values, coords = {
#    "datetime": da.datetime,
#    "variable": ["temperature"],
#    "ensemble": range(500)
#    })
#da.reindex_like(template)
```

## Adding weather covariates!

``` {python}
import pyarrow.dataset as ds
from pyarrow import fs

s3 = fs.S3FileSystem(endpoint_override = "data.ecoforecast.org", anonymous = True)
dataset = ds.dataset(
    "neon4cast-drivers/noaa/gefs-v12/stage1",
    format="parquet",
    filesystem=s3,
    partitioning=["start_date", "cycle"]
)
expression = (
              (ds.field("start_date") == "2022-04-01") &
              (ds.field("cycle") == 00) &
              (ds.field("ensemble") == 1)
              )
ex = dataset.to_table(filter=expression)
```




